{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ¤– Task B: Model Attribution Training - SemEval 2026 Task 13\n",
        "\n",
        "**Goal:** Train models to identify which AI model generated the code (11 classes)\n",
        "\n",
        "**Level:** â­â­â­ Advanced (2-3 hours)\n",
        "\n",
        "**What you'll learn:**\n",
        "- Multi-class classification (11 classes)\n",
        "- Model attribution problem\n",
        "- Handling class imbalance\n",
        "- Advanced evaluation metrics\n",
        "\n",
        "**Task B Details:**\n",
        "- **Classes:** 11 (10 AI models + Human)\n",
        "- **Expected Baseline:** 35-45% F1\n",
        "- **Target Performance:** 85%+ F1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Libraries loaded!\n",
            "ðŸ”’ Random seed: 42\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from src.features import extract_features_from_dataframe\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "import random\n",
        "random.seed(SEED)\n",
        "\n",
        "print(\"âœ… Libraries loaded!\")\n",
        "print(f\"ðŸ”’ Random seed: {SEED}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Task B Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âŒ Task B data not found!\n",
            "ðŸ“¥ Please download Task B data first:\n",
            "   python3 src/generate_data.py --task B\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../data/train_B.parquet'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load Task B datasets\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     train_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/train_B.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     val_df = pd.read_parquet(\u001b[33m'\u001b[39m\u001b[33m../data/validation_B.parquet\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Data loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/mlab/semeval26-task13-1/venv/lib/python3.12/site-packages/pandas/io/parquet.py:670\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    667\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    668\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m670\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/mlab/semeval26-task13-1/venv/lib/python3.12/site-packages/pandas/io/parquet.py:265\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    263\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    272\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    273\u001b[39m         path_or_handle,\n\u001b[32m    274\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    277\u001b[39m         **kwargs,\n\u001b[32m    278\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/mlab/semeval26-task13-1/venv/lib/python3.12/site-packages/pandas/io/parquet.py:139\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    129\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    131\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    132\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    137\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    143\u001b[39m     path_or_handle = handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/mlab/semeval26-task13-1/venv/lib/python3.12/site-packages/pandas/io/common.py:872\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    863\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    864\u001b[39m             handle,\n\u001b[32m    865\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    868\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    869\u001b[39m         )\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    871\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m872\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    873\u001b[39m     handles.append(handle)\n\u001b[32m    875\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/train_B.parquet'"
          ]
        }
      ],
      "source": [
        "# Load Task B datasets\n",
        "try:\n",
        "    train_df = pd.read_parquet('task_b_trial.parquet')\n",
        "    val_df = pd.read_parquet('../data/validation_B.parquet')\n",
        "    print(f\"âœ… Data loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ Task B data not found!\")\n",
        "    print(\"ðŸ“¥ Please download Task B data first:\")\n",
        "    print(\"   python3 src/generate_data.py --task B\")\n",
        "    raise\n",
        "\n",
        "print(f\"\\nTraining: {len(train_df):,} samples\")\n",
        "print(f\"Validation: {len(val_df):,} samples\")\n",
        "print(f\"\\nNumber of classes: {train_df['label'].nunique()}\")\n",
        "print(f\"Label range: {train_df['label'].min()} to {train_df['label'].max()}\")\n",
        "\n",
        "# Show label distribution\n",
        "print(\"\\nðŸ“Š Label Distribution (Training):\")\n",
        "label_counts = train_df['label'].value_counts().sort_index()\n",
        "print(label_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Visualize Class Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize class distribution\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Training distribution\n",
        "train_counts = train_df['label'].value_counts().sort_index()\n",
        "ax1.bar(range(len(train_counts)), train_counts.values, color='#42a5f5')\n",
        "ax1.set_xlabel('Class Label', fontsize=12)\n",
        "ax1.set_ylabel('Count', fontsize=12)\n",
        "ax1.set_title('Training Set - Class Distribution', fontsize=14, fontweight='bold')\n",
        "ax1.set_xticks(range(len(train_counts)))\n",
        "ax1.set_xticklabels(train_counts.index, rotation=45)\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Validation distribution\n",
        "val_counts = val_df['label'].value_counts().sort_index()\n",
        "ax2.bar(range(len(val_counts)), val_counts.values, color='#66bb6a')\n",
        "ax2.set_xlabel('Class Label', fontsize=12)\n",
        "ax2.set_ylabel('Count', fontsize=12)\n",
        "ax2.set_title('Validation Set - Class Distribution', fontsize=14, fontweight='bold')\n",
        "ax2.set_xticks(range(len(val_counts)))\n",
        "ax2.set_xticklabels(val_counts.index, rotation=45)\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Class balance check:\")\n",
        "print(f\"  Training - Min: {train_counts.min():,}, Max: {train_counts.max():,}, Ratio: {train_counts.max()/train_counts.min():.2f}x\")\n",
        "print(f\"  Validation - Min: {val_counts.min():,}, Max: {val_counts.max():,}, Ratio: {val_counts.max()/val_counts.min():.2f}x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Extract Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract features\n",
        "print(\"Extracting features...\")\n",
        "X_train = extract_features_from_dataframe(train_df)\n",
        "X_val = extract_features_from_dataframe(val_df)\n",
        "y_train = train_df['label'].values\n",
        "y_val = val_df['label'].values\n",
        "\n",
        "print(f\"âœ… Feature extraction complete!\")\n",
        "print(f\"Feature shape: {X_train.shape}\")\n",
        "print(f\"Number of features: {X_train.shape[1]}\")\n",
        "print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
        "print(f\"\\nFeature names: {list(X_train.columns)[:10]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Models\n",
        "\n",
        "**Note:** Task B is harder than Task A (11 classes vs 2). We'll use:\n",
        "- More trees for Random Forest (200 instead of 100)\n",
        "- Class balancing to handle potential imbalance\n",
        "- Multi-class evaluation metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define models with adjustments for multi-class problem\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(\n",
        "        max_iter=2000,  # More iterations for 11 classes\n",
        "        random_state=42,\n",
        "        class_weight='balanced',  # Handle class imbalance\n",
        "        multi_class='multinomial'  # Explicit multi-class\n",
        "    ),\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        n_estimators=200,  # More trees for complex 11-class problem\n",
        "        max_depth=20,\n",
        "        min_samples_split=10,\n",
        "        random_state=42,\n",
        "        class_weight='balanced',  # Handle class imbalance\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(\n",
        "        n_estimators=200,  # More trees\n",
        "        max_depth=5,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {name}...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Train\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predict\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_val = model.predict(X_val)\n",
        "    \n",
        "    # Evaluate\n",
        "    train_f1 = f1_score(y_train, y_pred_train, average='macro')\n",
        "    val_f1 = f1_score(y_val, y_pred_val, average='macro')\n",
        "    \n",
        "    results[name] = {\n",
        "        'train_f1': train_f1,\n",
        "        'val_f1': val_f1,\n",
        "        'model': model,\n",
        "        'predictions': y_pred_val\n",
        "    }\n",
        "    \n",
        "    print(f\"Train F1: {train_f1:.4f}\")\n",
        "    print(f\"Val F1:   {val_f1:.4f}\")\n",
        "    print(f\"\\nClassification Report (Top 5 classes shown):\")\n",
        "    # Show classification report for all classes\n",
        "    report = classification_report(y_val, y_pred_val, output_dict=True)\n",
        "    print(classification_report(y_val, y_pred_val))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': list(results.keys()),\n",
        "    'Train F1': [r['train_f1'] for r in results.values()],\n",
        "    'Val F1': [r['val_f1'] for r in results.values()]\n",
        "})\n",
        "\n",
        "print(comparison_df)\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "x = np.arange(len(comparison_df))\n",
        "width = 0.35\n",
        "\n",
        "ax.bar(x - width/2, comparison_df['Train F1'], width, label='Train F1', color='#66bb6a')\n",
        "ax.bar(x + width/2, comparison_df['Val F1'], width, label='Val F1', color='#42a5f5')\n",
        "\n",
        "ax.set_xlabel('Model', fontsize=12)\n",
        "ax.set_ylabel('Macro F1 Score', fontsize=12)\n",
        "ax.set_title('Task B: Model Comparison (11 Classes)', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(comparison_df['Model'], rotation=15, ha='right')\n",
        "ax.legend()\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add baseline expectation line\n",
        "ax.axhline(y=0.40, color='r', linestyle='--', alpha=0.5, label='Expected Baseline (40%)')\n",
        "ax.axhline(y=0.85, color='g', linestyle='--', alpha=0.5, label='Target (85%)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Confusion Matrix (Best Model)\n",
        "\n",
        "**Note:** For 11 classes, the confusion matrix will be 11x11. This shows which models are confused with each other.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get best model\n",
        "best_model_name = max(results, key=lambda k: results[k]['val_f1'])\n",
        "best_predictions = results[best_model_name]['predictions']\n",
        "\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "print(f\"Validation F1: {results[best_model_name]['val_f1']:.4f}\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_val, best_predictions)\n",
        "\n",
        "# Create labels for 11 classes\n",
        "class_labels = [f'Class {i}' for i in range(11)]\n",
        "# If you know the actual model names, replace above with:\n",
        "# class_labels = ['Human', 'GPT-3.5', 'GPT-4', 'Claude', 'Codex', ...]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=class_labels,\n",
        "            yticklabels=class_labels,\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title(f'Confusion Matrix - {best_model_name} (Task B)', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print per-class accuracy\n",
        "print(\"\\nðŸ“Š Per-Class Accuracy:\")\n",
        "for i in range(11):\n",
        "    class_mask = y_val == i\n",
        "    if class_mask.sum() > 0:\n",
        "        class_acc = (best_predictions[class_mask] == i).mean()\n",
        "        print(f\"  Class {i}: {class_acc:.4f} ({class_mask.sum()} samples)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Feature Importance (Random Forest)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importance from Random Forest\n",
        "rf_model = results['Random Forest']['model']\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Plot top 15 features\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_features = feature_importance.head(15)\n",
        "plt.barh(range(len(top_features)), top_features['importance'], color='#66bb6a')\n",
        "plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "plt.xlabel('Importance', fontsize=12)\n",
        "plt.title('Top 15 Most Important Features (Task B)', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 10 Features:\")\n",
        "print(feature_importance.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Your Turn! ðŸŽ¯\n",
        "\n",
        "**Experiments to try for Task B:**\n",
        "1. **Hyperparameter tuning** - More trees, different depths\n",
        "2. **Feature engineering** - Model-specific features (different AI models may have distinct patterns)\n",
        "3. **Class balancing** - Try different class_weight strategies\n",
        "4. **Advanced models** - XGBoost, LightGBM often perform better on multi-class\n",
        "5. **Ensemble methods** - Combine multiple models\n",
        "6. **AST features** - Structural patterns may differ between AI models\n",
        "\n",
        "**Add your code below:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your experiments here!\n",
        "# Example: Try XGBoost\n",
        "# from xgboost import XGBClassifier\n",
        "# xgb_model = XGBClassifier(n_estimators=200, random_state=42)\n",
        "# xgb_model.fit(X_train, y_train)\n",
        "# xgb_pred = xgb_model.predict(X_val)\n",
        "# xgb_f1 = f1_score(y_val, xgb_pred, average='macro')\n",
        "# print(f\"XGBoost F1: {xgb_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Key Takeaways\n",
        "\n",
        "**Expected baseline performance (Task B):**\n",
        "- Logistic Regression: ~30-40% F1\n",
        "- Random Forest: ~35-45% F1\n",
        "- Gradient Boosting: ~35-45% F1\n",
        "\n",
        "**Why Task B is harder:**\n",
        "- 11 classes vs 2 classes (more complex decision boundaries)\n",
        "- Different AI models may have similar patterns\n",
        "- Requires more sophisticated features to distinguish models\n",
        "\n",
        "**To reach competitive performance (85%+ F1):**\n",
        "- Add AST features (structural patterns differ between models)\n",
        "- Use transformer models like CodeBERT\n",
        "- Model-specific feature engineering\n",
        "- Ensemble multiple models\n",
        "- Hyperparameter optimization\n",
        "\n",
        "**Your observations:**\n",
        "- \n",
        "- \n",
        "- \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœ… Next Steps\n",
        "\n",
        "1. **Analyze confusion matrix** - Which models are confused with each other?\n",
        "2. **Add AST features** - Structural patterns may help distinguish AI models\n",
        "3. **Try advanced models** - XGBoost, LightGBM for better multi-class performance\n",
        "4. **Feature engineering** - Create model-specific features\n",
        "5. **Hyperparameter tuning** - Optimize for 11-class problem\n",
        "\n",
        "**Great work on training Task B models!** ðŸŽ‰\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
