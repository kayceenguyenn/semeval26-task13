{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B Training - Model Attribution (12-class)\n",
    "\n",
    "This notebook trains an improved model for Task B using:\n",
    "- TF-IDF features (character + word n-grams)\n",
    "- Cosine similarity to class centroids\n",
    "- AST-based structural features\n",
    "- XGBoost with GPU acceleration\n",
    "\n",
    "**Target:** Improve from ~20% to 40-55% Macro F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import sys\n",
    "import os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Clone repo (only if not already cloned)\n",
    "    if not os.path.exists('/content/semeval26-task13'):\n",
    "        !git clone https://github.com/kayceenguyenn/semeval26-task13.git /content/semeval26-task13\n",
    "    \n",
    "    # Always change to repo directory (use absolute path)\n",
    "    %cd /content/semeval26-task13\n",
    "    \n",
    "    # Install only the packages we need (without strict versions to avoid Colab conflicts)\n",
    "    !pip install -q xgboost loguru tqdm pydantic pydantic-settings\n",
    "    \n",
    "    # Copy data from Google Drive\n",
    "    !mkdir -p data\n",
    "    !cp /content/drive/MyDrive/semeval-data/*.parquet data/ 2>/dev/null || echo \"Data already copied or not found in Drive\"\n",
    "    \n",
    "    print(\"Running in Google Colab\")\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, 'src' if Path('src').exists() else '../src')\n",
    "\n",
    "from data_loader import TaskDataLoader\n",
    "from features import (\n",
    "    extract_features_from_dataframe,\n",
    "    fit_tfidf_pipeline,\n",
    "    save_fitted_state,\n",
    "    load_fitted_state,\n",
    ")\n",
    "from models import get_model\n",
    "from evaluate import evaluate, print_results\n",
    "\n",
    "# Check GPU availability\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(f\"XGBoost version: {xgb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"XGBoost not installed!\")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Task B data\n",
    "loader = TaskDataLoader(task='B')\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "train_df = loader.load_split('train')\n",
    "print(f\"Train samples: {len(train_df):,}\")\n",
    "\n",
    "print(\"\\nLoading validation data...\")\n",
    "val_df = loader.load_split('validation')\n",
    "print(f\"Validation samples: {len(val_df):,}\")\n",
    "\n",
    "# Show label distribution\n",
    "print(\"\\nLabel distribution (train):\")\n",
    "print(train_df['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample for faster iteration (optional)\n",
    "SAMPLE_SIZE = None  # Set to e.g. 50000 for faster testing, None for full dataset\n",
    "\n",
    "if SAMPLE_SIZE:\n",
    "    print(f\"Sampling {SAMPLE_SIZE:,} training samples for faster iteration...\")\n",
    "    train_df = train_df.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "    print(f\"Sampled train size: {len(train_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fit TF-IDF Pipeline\n",
    "\n",
    "This fits the TF-IDF vectorizers and computes class centroids on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit TF-IDF on training data\n",
    "train_codes = train_df['code'].tolist()\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "fit_tfidf_pipeline(train_codes, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fitted state for later use\n",
    "FITTED_STATE_PATH = 'models/tfidf_state'\n",
    "save_fitted_state(FITTED_STATE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from training data (with TF-IDF)\n",
    "print(\"Extracting training features...\")\n",
    "X_train = extract_features_from_dataframe(train_df, include_tfidf=True)\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "print(f\"\\nTraining features shape: {X_train.shape}\")\n",
    "print(f\"Feature columns: {list(X_train.columns[:10])}... (and {len(X_train.columns)-10} more)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from validation data\n",
    "print(\"Extracting validation features...\")\n",
    "X_val = extract_features_from_dataframe(val_df, include_tfidf=True)\n",
    "y_val = val_df['label'].values\n",
    "\n",
    "print(f\"Validation features shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train XGBoost model\n",
    "model = get_model(\n",
    "    'xgboost',\n",
    "    n_estimators=300,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    use_gpu=True,  # Use GPU if available\n",
    ")\n",
    "\n",
    "# Train with validation set for early stopping\n",
    "model.fit(X_train, y_train, X_val=X_val, y_val=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "results = model.evaluate(X_val, y_val, detailed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature importance\n",
    "model.get_feature_importance(list(X_train.columns), top_n=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "MODEL_PATH = 'models/task_B_xgboost.pkl'\n",
    "model.save(MODEL_PATH)\n",
    "\n",
    "print(f\"\\nModel saved to: {MODEL_PATH}\")\n",
    "print(f\"TF-IDF state saved to: {FITTED_STATE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy to Google Drive (if in Colab)\n",
    "if IN_COLAB:\n",
    "    !cp -r models /content/drive/MyDrive/semeval-models/\n",
    "    print(\"Models copied to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline for comparison (without TF-IDF features)\n",
    "print(\"Training baseline Random Forest for comparison...\")\n",
    "print(\"(Using only basic + keyword + AST features)\")\n",
    "\n",
    "# Extract features without TF-IDF\n",
    "X_train_basic = extract_features_from_dataframe(train_df, include_tfidf=False)\n",
    "X_val_basic = extract_features_from_dataframe(val_df, include_tfidf=False)\n",
    "\n",
    "baseline = get_model('random_forest')\n",
    "baseline.fit(X_train_basic, y_train)\n",
    "\n",
    "print(\"\\nBaseline Results:\")\n",
    "baseline_results = baseline.evaluate(X_val_basic, y_val, detailed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBaseline (RF, {X_train_basic.shape[1]} features):  {baseline_results['macro_f1']:.4f} Macro F1\")\n",
    "print(f\"XGBoost  ({X_train.shape[1]} features):  {results['macro_f1']:.4f} Macro F1\")\n",
    "print(f\"\\nImprovement: +{(results['macro_f1'] - baseline_results['macro_f1'])*100:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Make Predictions on Test Set (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data (if available)\n",
    "try:\n",
    "    test_df = loader.load_split('test')\n",
    "    print(f\"Test samples: {len(test_df):,}\")\n",
    "    \n",
    "    # Extract features\n",
    "    X_test = extract_features_from_dataframe(test_df, include_tfidf=True)\n",
    "    \n",
    "    # Make predictions\n",
    "    test_preds = model.predict(X_test)\n",
    "    \n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'label': test_preds\n",
    "    })\n",
    "    \n",
    "    submission.to_csv('results/predictions/task_B_submission.csv', index=False)\n",
    "    print(\"\\nSubmission saved to results/predictions/task_B_submission.csv\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Test data not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "To further improve performance:\n",
    "\n",
    "1. **Hyperparameter tuning**: Try different `n_estimators`, `max_depth`, `learning_rate`\n",
    "2. **More TF-IDF features**: Increase `max_features` in vectorizers\n",
    "3. **LightGBM**: Often faster and comparable accuracy\n",
    "4. **Ensemble**: Combine XGBoost with Random Forest\n",
    "5. **Transformers**: Fine-tune CodeBERT for maximum performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
